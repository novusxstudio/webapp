# RL Self-Play Configuration
# ===========================

# Environment settings
env:
  grid_size: 5
  max_steps_per_game: 2000      # Allow for 1000 turns (2 steps per turn)
  unit_types: ["Swordsman", "Shieldman", "Axeman", "Cavalry", "Archer", "Spearman"]

# Training settings
training:
  total_episodes: 100000
  checkpoint_interval: 1000
  log_interval: 100
  seed: 42
  
  # PPO hyperparameters
  ppo:
    learning_rate: 3.0e-4
    gamma: 0.99                 # Discount factor
    gae_lambda: 0.95            # GAE lambda for advantage estimation
    clip_epsilon: 0.2           # PPO clipping parameter
    value_loss_coef: 0.5        # Value loss coefficient
    entropy_coef: 0.01          # Entropy bonus coefficient
    max_grad_norm: 0.5          # Gradient clipping
    n_epochs: 4                 # PPO epochs per update
    batch_size: 64              # Mini-batch size
    rollout_steps: 128          # Steps before each update

# Neural network architecture
model:
  hidden_size: 256
  num_layers: 3
  dropout: 0.1

# =============================================================================
# REWARD CONFIGURATION (Exploration + Rewards + Draws + Stability)
# =============================================================================
# Terminal rewards (assigned exactly once at game termination):
#   Win:  +1.0 for winner, -1.0 for loser
#   Draw:  0.0 for both players
# Shaping rewards for irreversible progress only
rewards:
  # ─────────────────────────────────────────────────────────────────────────
  # TERMINAL REWARDS (DOMINANT - assigned exactly once at game end)
  # ─────────────────────────────────────────────────────────────────────────
  win: 1.0                      # Winner receives +1.0
  lose: -1.0                    # Loser receives -1.0
  
  # ─────────────────────────────────────────────────────────────────────────
  # DRAW REWARDS BY TYPE (Training only - evaluation uses 0.0 for all draws)
  # ─────────────────────────────────────────────────────────────────────────
  # Different draw types receive different penalties to shape agent behavior:
  # - no_progress: Neutral (0.0) - stalemate is acceptable, not agent's fault
  # - repetition: Slight penalty (-0.2) - agents should avoid repeating states
  # - max_turns: Moderate penalty (-0.3) - agents should resolve games faster
  # All draw penalties are LESS severe than a loss (-1.0) to avoid
  # risk-averse behavior that prefers losing over drawing.
  draw_no_progress: 0.0         # No captures/deaths for N turns - neutral
  draw_repetition: -0.2         # Repeated game states - slight penalty
  draw_max_turns: -0.3          # Exceeded turn limit - moderate penalty
  draw_default: 0.0             # Fallback for evaluation mode
  
  # ─────────────────────────────────────────────────────────────────────────
  # PER-TURN PENALTIES (prevents stalling)
  # ─────────────────────────────────────────────────────────────────────────
  turn_penalty: -0.001          # Small penalty each non-terminal turn
  
  # ─────────────────────────────────────────────────────────────────────────
  # OBJECTIVE REWARDS (Control Points - irreversible progress)
  # ─────────────────────────────────────────────────────────────────────────
  capture_control_point: 0.3    # +0.3 per control point captured
  lose_control_point: -0.3      # -0.3 per control point lost
  control_point_advantage: 0.02 # +0.02 × (our_cp - enemy_cp) per turn
  
  # ─────────────────────────────────────────────────────────────────────────
  # COMBAT REWARDS (Unit defeats - irreversible progress)
  # ─────────────────────────────────────────────────────────────────────────
  defeat_enemy_unit: 0.07       # +0.07 per enemy unit defeated
  lose_own_unit: -0.07          # -0.07 per own unit lost
  
  # Invalid action penalty
  invalid_action_penalty: -0.5

# =============================================================================
# EXPLORATION SETTINGS (Temperature-based policy sampling)
# =============================================================================
exploration:
  # Temperature-based exploration during training only
  # Higher temperature = more exploration, lower = more exploitation
  temperature_early: 1.5        # Temperature for turns 1-10
  temperature_late: 0.7         # Temperature for turns > 10
  temperature_threshold: 10     # Turn number to switch temperatures
  
  # During evaluation/inference: always use argmax (deterministic)
  # Illegal actions: always masked to zero probability before sampling

# Draw detection settings (configured in env.py)
# ==============================================
# MAX_TURN_LIMIT: 1000          # turnCount >= 1000 → DRAW
# REPEATED_STATE_LIMIT: 10      # Same state hash 10 times → DRAW
# NO_PROGRESS_TURN_LIMIT: 100   # No capture AND no death for 100 turns → DRAW

# Self-play settings (fixed-role, no weight copying)
self_play:
  elo_update_k: 32              # ELO K-factor for rating updates
  win_rate_window: 100          # Window for calculating win rates

# Evaluation settings
evaluation:
  num_eval_games: 100
  render_games: false

# Paths
paths:
  checkpoint_dir: "rl/checkpoints"
  log_dir: "rl/logs"
